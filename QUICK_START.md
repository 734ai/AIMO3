# AIMO3 Quick Reference Card

## ğŸš€ One-Line Commands

### Test Single Model (GPT-2)
```bash
python src/task_3_5_kaggle_submission.py --model gpt2 --test datasets/aimo3_test.csv
```

### Test Ensemble
```bash
python -c "from src.task_3_6_ensemble import EnsembleInferencer; \
e=EnsembleInferencer(); e.load_models(['gpt2']); \
r=e.predict_ensemble('What is 2+2?'); print(r['final_answer'])"
```

### Show Available Models
```bash
python src/multi_model_support.py
```

### Run Kaggle Notebook
- Copy `notebooks/aimo3_kaggle_ready.ipynb` to Kaggle
- Add dataset: `ai-mathematical-olympiad-progress-prize-3`
- Run all cells

---

## ğŸ“Š Model Quick Select

| Need | Model | Command | Time/Problem | VRAM |
|------|-------|---------|--------------|------|
| **Test** | GPT-2 | `gpt2` | instant | 1GB |
| **Fast** | Gemma 3 4B | `gemma3-4b` | 3s | 8GB |
| **Balanced** â­ | Gemma 3 12B | `gemma3-12b` | 6s | 24GB |
| **Strong** | Qwen 3 32B | `qwen3-32b` | 15s | 64GB |
| **Best** | DeepSeek-R1 | `deepseek-r1` | 45s | 24GB* |

*with int8 quantization

---

## ğŸ’¾ Key Files

| File | Purpose | Lines |
|------|---------|-------|
| `src/multi_model_support.py` | 13 model registry | 450 |
| `src/task_3_6_ensemble.py` | Ensemble voting | 400+ |
| `notebooks/aimo3_kaggle_ready.ipynb` | Ready-to-run submission | 17 cells |
| `MODEL_SELECTION_GUIDE.md` | Detailed guide | 600+ |
| `MULTI_MODEL_DEPLOYMENT.md` | Deployment guide | 550+ |

---

## ğŸ¯ Expected Results

```
Model              Time/8000   VRAM    Accuracy
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Gemma 3 12B        13h        24GB    20-25%  âœ“ RECOMMENDED
Qwen 3 32B         20h        64GB    25-35%
2-Model Ensemble   20h        48GB    25-35%
3-Model Ensemble   30h        100GB   35-45%
DeepSeek-R1        45h        24GB*   30-45%
```

---

## ğŸ”§ In-Notebook Usage

### Load and Infer
```python
from notebooks.aimo3_kaggle_ready import AIOMInference

inferencer = AIOMInference(model_name="gemma3-12b")
inferencer.load_model()
answer = inferencer.generate("What is 2+2?")
print(answer)
```

### Ensemble
```python
from src.task_3_6_ensemble import EnsembleInferencer

ensemble = EnsembleInferencer()
ensemble.load_models(["gemma3-12b", "qwen3-32b"])
result = ensemble.predict_ensemble(problem)
print(result["final_answer"])
```

---

## âš¡ Performance Tips

1. **GPU Memory**: Use int8 quantization for large models
2. **Speed**: Start with Gemma 3 4B or 12B
3. **Accuracy**: Use Qwen 3 32B or ensemble
4. **Cost**: Free open-source models recommended

---

## ğŸ“ Architecture

```
Input Problem
    â†“
Model Selection
    â”œâ†’ Single Model (Fast)
    â”œâ†’ Ensemble (Accurate)
    â””â†’ Fine-tuned (Best)
    â†“
Chain-of-Thought Prompt
    â†“
Generation (temperature=0.7, max_tokens=512)
    â†“
Answer Extraction (regex)
    â†“
Kaggle Submission CSV
```

---

## ğŸ“ˆ Iteration Strategy

**Phase 1** (30 min): Test pipeline with GPT-2
**Phase 2** (13h): Run Gemma 3 12B, submit to Kaggle
**Phase 3** (iterative): Add ensemble if time permits
**Phase 4** (optional): Fine-tune for +10% improvement

---

## âœ… Checklist

- [x] Multi-model registry (13 models)
- [x] Ensemble inference engine
- [x] Kaggle notebook updated
- [x] Test data integrated
- [x] Submission format verified
- [x] Documentation complete
- [ ] Upload to Kaggle â† NEXT STEP
- [ ] Run on full test set
- [ ] Monitor leaderboard

---

**Status: ğŸŸ¢ READY TO DEPLOY**

Next action: Copy notebook to Kaggle and run! ğŸš€
