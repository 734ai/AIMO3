import json
import os

NOTEBOOK_PATH = "notebooks/aimo3_kaggle_ready.ipynb"

def update_notebook():
    with open(NOTEBOOK_PATH, 'r') as f:
        nb = json.load(f)

    # 1. Update AIOMInference class to handle local models (id: a4d3f7b9)
    new_inference_code = [
        "class AIOMInference:\n",
        "    \"\"\"Multi-model inference engine with model selection\"\"\"\n",
        "    \n",
        "    # Supported models\n",
        "    SUPPORTED_MODELS = {\n",
        "        \"gpt2\": {\"name\": \"GPT-2 (Fast Baseline)\", \"vram\": 1, \"speed\": \"instant\"},\n",
        "        \"gemma3-4b\": {\"name\": \"Gemma 3 4B\", \"vram\": 8, \"speed\": \"3s/q\"},\n",
        "        \"gemma3-12b\": {\"name\": \"Gemma 3 12B\", \"vram\": 24, \"speed\": \"6s/q\"},\n",
        "        \"llama4-scout\": {\"name\": \"Llama 4 Scout 8B\", \"vram\": 16, \"speed\": \"8s/q\"},\n",
        "        \"qwen3-32b\": {\"name\": \"Qwen 3 32B\", \"vram\": 64, \"speed\": \"15s/q\"},\n",
        "        \"deepseek-r1\": {\"name\": \"DeepSeek-R1 67B\", \"vram\": 180, \"speed\": \"45s/q\", \"reasoning\": True},\n",
        "        \"mistral-large-3\": {\"name\": \"Mistral Large 3 123B\", \"vram\": 280, \"speed\": \"50s/q\"},\n",
        "    }\n",
        "    \n",
        "    def __init__(self, model_name=\"gpt2\", lora_path=None, device=\"cuda\"):\n",
        "        self.device = device\n",
        "        self.model_name = model_name\n",
        "        self.lora_path = lora_path\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        \n",
        "    def load_model(self):\n",
        "        \"\"\"Load model checking local paths first for offline Kaggle use\"\"\"\n",
        "        print(f\"Loading {self.SUPPORTED_MODELS.get(self.model_name, {}).get('name', self.model_name)}...\")\n",
        "        \n",
        "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "        from peft import PeftModel\n",
        "        \n",
        "        # Check for local model paths (Kaggle input datasets)\n",
        "        # Priority: 1. Exact path, 2. /kaggle/input/{name}, 3. Recursive search\n",
        "        \n",
        "        def find_model_path_recursive(base_path=\"/kaggle/input\", model_identifier=\"gpt2\"):\n",
        "            \"\"\"Recursively search for model directory containing specific identifier\"\"\"\n",
        "            try:\n",
        "                # First check common paths specific to this competition/user\n",
        "                sys_paths = [\n",
        "                    f\"/kaggle/input/{model_identifier}\",\n",
        "                    f\"/kaggle/input/model-{model_identifier}\",\n",
        "                    f\"/kaggle/input/datasets/muzansano/model-{model_identifier}\",\n",
        "                    f\"/kaggle/input/datasets/muzansano/{model_identifier}\",\n",
        "                ]\n",
        "                for p in sys_paths:\n",
        "                    if os.path.exists(p):\n",
        "                        return p\n",
        "\n",
        "                # Fallback: Recursive search\n",
        "                if os.path.exists(base_path):\n",
        "                    for root, dirs, files in os.walk(base_path):\n",
        "                        if model_identifier in root.split(os.sep):\n",
        "                            return root\n",
        "                        for d in dirs:\n",
        "                            if model_identifier in d or f\"model-{model_identifier}\" in d:\n",
        "                                return os.path.join(root, d)\n",
        "            except:\n",
        "                pass\n",
        "            return None\n",
        "\n",
        "        # Try to find model path dynamically\n",
        "        found_path = find_model_path_recursive(model_identifier=self.model_name.split('/')[-1])\n",
        "        if found_path:\n",
        "             print(f\"‚úÖ Dynamically found model path: {found_path}\")\n",
        "        \n",
        "        potential_paths = [\n",
        "            self.model_name,\n",
        "            found_path if found_path else f\"/kaggle/input/{self.model_name.split('/')[-1]}\",\n",
        "            f\"/kaggle/input/{self.model_name.split('/')[-1]}\",\n",
        "            f\"/kaggle/input/model-{self.model_name.split('/')[-1]}\",\n",
        "            f\"/kaggle/input/{self.model_name.replace('/', '-')}\"\n",
        "        ]\n",
        "        \n",
        "        model_path = self.model_name\n",
        "        local_files_only = False\n",
        "        \n",
        "        for path in potential_paths:\n",
        "            if os.path.exists(path):\n",
        "                print(f\"‚úÖ Found local model at: {path}\")\n",
        "                model_path = path\n",
        "                local_files_only = True\n",
        "                break\n",
        "        \n",
        "        try:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=local_files_only)\n",
        "            self.model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
        "                device_map=\"auto\" if self.device == \"cuda\" else None,\n",
        "                local_files_only=local_files_only\n",
        "            )\n",
        "            \n",
        "            # Load LoRA weights if available\n",
        "            if self.lora_path and os.path.exists(self.lora_path):\n",
        "                print(f\"Loading LoRA from {self.lora_path}...\")\n",
        "                self.model = PeftModel.from_pretrained(self.model, self.lora_path)\n",
        "            \n",
        "            if self.device == \"cpu\":\n",
        "                self.model = self.model.to(self.device)\n",
        "            \n",
        "            self.model.eval()\n",
        "            if self.tokenizer.pad_token is None:\n",
        "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "            \n",
        "            print(\"‚úÖ Model loaded!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load model: {e}\")\n",
        "            raise e\n",
        "    \n",
        "    def create_prompt(self, problem, include_cot=True):\n",
        "        \"\"\"Create chain-of-thought prompt\"\"\"\n",
        "        if include_cot:\n",
        "            return f\"\"\"Solve this mathematical problem step by step.\n",
        "\n",
        "Problem: {problem}\n",
        "\n",
        "Let's think through this:\n",
        "1. What is being asked?\n",
        "2. What approach should we use?\n",
        "3. Let's work through the solution\n",
        "4. Extract the numerical answer\n",
        "\n",
        "Answer:\"\"\"\n",
        "        else:\n",
        "            return f\"Problem: {problem}\\n\\nAnswer:\"\n",
        "    \n",
        "    def extract_answer(self, text):\n",
        "        \"\"\"Extract numeric answer from generated text\"\"\"\n",
        "        # Try patterns like \"Answer: 42\"\n",
        "        match = re.search(r'(?:Answer|Final|Result)[:\\s]*([-\\d.]+)', text, re.IGNORECASE)\n",
        "        if match:\n",
        "            return match.group(1)\n",
        "        \n",
        "        # Extract last number\n",
        "        numbers = re.findall(r'-?\\d+(?:\\.\\d+)?', text)\n",
        "        if numbers:\n",
        "            return numbers[-1]\n",
        "        \n",
        "        return \"0\"\n",
        "    \n",
        "    def generate(self, problem, max_length=512, temperature=0.7):\n",
        "        \"\"\"Generate answer for problem\"\"\"\n",
        "        prompt = self.create_prompt(problem, include_cot=True)\n",
        "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                inputs,\n",
        "                max_length=max_length,\n",
        "                temperature=temperature,\n",
        "                top_p=0.9,\n",
        "                do_sample=True,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "        \n",
        "        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        answer = self.extract_answer(text.split(\"Answer:\")[-1])\n",
        "        return answer\n",
        "    \n",
        "    @staticmethod\n",
        "    def list_available_models():\n",
        "        \"\"\"List all available models\"\"\"\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"AVAILABLE MODELS\")\n",
        "        print(\"=\"*80)\n",
        "        for key, info in AIOMInference.SUPPORTED_MODELS.items():\n",
        "            reasoning = \" [Reasoning]\" if info.get(\"reasoning\") else \"\"\n",
        "            print(f\"{key:<20} | {info['name']:<40} | {info['vram']:<6}GB | {info['speed']}{reasoning}\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "print(\"‚úÖ Inference class updated for Offline/Kaggle use\")"
    ]

    # 2. Update Data Loading to Hybrid (id: 5365a938)
    new_data_loading_code = [
        "# 5. Check environment and initialize data loader\n",
        "try:\n",
        "    import aimo\n",
        "    env = aimo.make_env()\n",
        "    iter_test = env.iter_test()\n",
        "    KAGGLE_MODE = True\n",
        "    print(\"‚úÖ Kaggle AIMO environment initialized\")\n",
        "except ImportError:\n",
        "    KAGGLE_MODE = False\n",
        "    print(\"‚ö†Ô∏è AIMO API not found - running in LOCAL/DEBUG mode\")\n",
        "    \n",
        "    # Check available test data sources for local debugging\n",
        "    test_paths = [\n",
        "        \"datasets/aimo3_test.csv\",  # Local path (absolute)\n",
        "        \"../datasets/aimo3_test.csv\",  # Local path (relative)\n",
        "        \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv\"  # Kaggle public data\n",
        "    ]\n",
        "    \n",
        "    test_path = None\n",
        "    for path in test_paths:\n",
        "        if os.path.exists(path):\n",
        "            test_path = path\n",
        "            break\n",
        "            \n",
        "    if test_path:\n",
        "        print(f\"Loading local test data from: {test_path}\")\n",
        "        test_df = pd.read_csv(test_path)\n",
        "        print(f\"‚úÖ Loaded {len(test_df)} test problems\")\n",
        "    else:\n",
        "        # Create dummy data if no file found\n",
        "        print(\"‚ö†Ô∏è No test data found. Creating dummy data.\")\n",
        "        test_df = pd.DataFrame([\n",
        "            {\"id\": \"000aaa\", \"problem\": \"What is $1+1$?\"},\n",
        "            {\"id\": \"111bbb\", \"problem\": \"Solve $x^2=4$ for positive $x$.\"},\n",
        "            {\"id\": \"222ccc\", \"problem\": \"Find the sum of angles in a triangle.\"}\n",
        "        ])\n"
    ]

    # 3. Update Prediction Loop with Integrated Verification (id: 78aa4489)
    new_prediction_code = [
        "# 6. Core Classes & Prediction Loop\n",
        "import time\n",
        "import re\n",
        "import sympy as sp\n",
        "from typing import Any, Optional, Dict, Tuple, Union\n",
        "\n",
        "# --- Core Classes (Injected for Kaggle Standalone) ---\n",
        "\n",
        "class SymbolicCompute:\n",
        "    \"\"\"SymPy-based symbolic computation and verification.\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def evaluate_expression(expr_str: str) -> Optional[Union[int, float]]:\n",
        "        \"\"\"Evaluate a mathematical expression string using SymPy.\"\"\"\n",
        "        try:\n",
        "            expr = sp.sympify(expr_str)\n",
        "            result = expr.evalf()\n",
        "            return int(result) if result == int(result) else float(result)\n",
        "        except Exception as e:\n",
        "            return None\n",
        "    \n",
        "    @staticmethod\n",
        "    def parse_llm_output_for_expressions(llm_text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Parse LLM output to extract mathematical expressions.\"\"\"\n",
        "        result = {\n",
        "            \"expressions\": [],\n",
        "            \"values\": [],\n",
        "            \"final_value\": None\n",
        "        }\n",
        "        try:\n",
        "            # Extract expressions\n",
        "            expr_pattern = r\"(?:=|equals|is)\\s*(\\d+(?:\\.\\d+)?|[\\w\\s\\+\\-\\*\/\\(\\)\\.]+)\"\n",
        "            expr_matches = re.findall(expr_pattern, llm_text, re.IGNORECASE)\n",
        "            \n",
        "            # Extract numeric values\n",
        "            num_pattern = r\"\\b(\\d+(?:\\.\\d+)?)\\b\"\n",
        "            num_matches = re.findall(num_pattern, llm_text)\n",
        "            \n",
        "            result[\"expressions\"] = expr_matches[:5]\n",
        "            result[\"values\"] = [float(n) if '.' in n else int(n) for n in num_matches[:10]]\n",
        "            \n",
        "            if num_matches:\n",
        "                result[\"final_value\"] = float(num_matches[-1]) if '.' in num_matches[-1] else int(num_matches[-1])\n",
        "        except:\n",
        "            pass\n",
        "        return result\n",
        "    \n",
        "    @staticmethod\n",
        "    def verify_symbolic_result(\n",
        "        llm_answer: int,\n",
        "        llm_output: str,\n",
        "        tolerance: float = 0.01\n",
        "    ) -> Tuple[bool, float]:\n",
        "        \"\"\"Verify LLM answer by symbolic computation. Returns (is_valid, confidence).\"\"\"\n",
        "        try:\n",
        "            parsed = SymbolicCompute.parse_llm_output_for_expressions(llm_output)\n",
        "            \n",
        "            # Try to evaluate extracted expressions\n",
        "            if parsed[\"expressions\"]:\n",
        "                for expr_str in parsed[\"expressions\"]:\n",
        "                    try:\n",
        "                        result = SymbolicCompute.evaluate_expression(expr_str)\n",
        "                        if result is not None:\n",
        "                            if isinstance(result, float):\n",
        "                                diff_percent = abs(result - llm_answer) / max(abs(llm_answer), 1)\n",
        "                                if diff_percent <= tolerance:\n",
        "                                    return True, 1.0 - diff_percent\n",
        "                            else:\n",
        "                                if int(result) == llm_answer:\n",
        "                                    return True, 1.0\n",
        "                    except:\n",
        "                        continue\n",
        "            \n",
        "            # Check final value\n",
        "            if parsed[\"final_value\"] is not None and parsed[\"final_value\"] == llm_answer:\n",
        "                return True, 0.8\n",
        "                    \n",
        "        except:\n",
        "            pass\n",
        "        return False, 0.5\n",
        "\n",
        "class AnswerValidator:\n",
        "    \"\"\"Validates and enforces answer format constraints.\"\"\"\n",
        "    \n",
        "    AIMO_MIN = 0\n",
        "    AIMO_MAX = 99999\n",
        "    \n",
        "    @staticmethod\n",
        "    def validate_integer(value: Any) -> Optional[int]:\n",
        "        \"\"\"Validate and convert value to valid AIMO integer.\"\"\"\n",
        "        try:\n",
        "            int_value = int(float(str(value).strip()))\n",
        "            if int_value < AnswerValidator.AIMO_MIN: return AnswerValidator.AIMO_MIN\n",
        "            if int_value > AnswerValidator.AIMO_MAX: return AnswerValidator.AIMO_MAX\n",
        "            return int_value\n",
        "        except:\n",
        "            return None\n",
        "    \n",
        "    @staticmethod\n",
        "    def validate_with_fallback_strategies(\n",
        "        llm_answer: Optional[int],\n",
        "        llm_text: str\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Validate answer with multiple fallback strategies.\"\"\"\n",
        "        result = {\n",
        "            \"final_answer\": 0,\n",
        "            \"confidence\": 0.0,\n",
        "            \"strategy_used\": \"default_fallback\",\n",
        "            \"fallback_applied\": False\n",
        "        }\n",
        "        try:\n",
        "            # Strategy 1: Use primary answer if valid\n",
        "            if llm_answer is not None:\n",
        "                validated = AnswerValidator.validate_integer(llm_answer)\n",
        "                if validated is not None:\n",
        "                    result[\"final_answer\"] = validated\n",
        "                    result[\"confidence\"] = 0.9\n",
        "                    result[\"strategy_used\"] = \"primary_llm_answer\"\n",
        "                    return result\n",
        "            \n",
        "            # Strategy 2: Try symbolic verification\n",
        "            is_valid, confidence = SymbolicCompute.verify_symbolic_result(\n",
        "                llm_answer if llm_answer is not None else 0,\n",
        "                llm_text\n",
        "            )\n",
        "            if is_valid and llm_answer is not None:\n",
        "                validated = AnswerValidator.validate_integer(llm_answer)\n",
        "                if validated is not None:\n",
        "                    result[\"final_answer\"] = validated\n",
        "                    result[\"confidence\"] = confidence\n",
        "                    result[\"strategy_used\"] = \"symbolic_verification\"\n",
        "                    result[\"fallback_applied\"] = True\n",
        "                    return result\n",
        "            \n",
        "            result[\"fallback_applied\"] = True\n",
        "        except:\n",
        "            pass\n",
        "        return result\n",
        "    \n",
        "    @staticmethod\n",
        "    def handle_edge_cases(answer: int) -> Tuple[int, str]:\n",
        "        \"\"\"Handle edge cases in answer validation.\"\"\"\n",
        "        try:\n",
        "            if answer < 0: return 0, \"Negative answer converted to 0\"\n",
        "            if answer > AnswerValidator.AIMO_MAX * 10: \n",
        "                return AnswerValidator.AIMO_MAX, \"Very large answer capped\"\n",
        "            validated = AnswerValidator.validate_integer(answer)\n",
        "            return validated if validated is not None else 0, \"\"\n",
        "        except:\n",
        "            return 0, \"Edge case error\"\n",
        "\n",
        "class ExecutionMetrics:\n",
        "    \"\"\"Track execution metrics for the pipeline.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.metrics = {\n",
        "            \"total\": 0, \"successful\": 0, \"failed\": 0, \n",
        "            \"fallback\": 0, \"verified\": 0\n",
        "        }\n",
        "    \n",
        "    def record_result(self, success: bool, fallback_used: bool = False, verified: bool = False):\n",
        "        self.metrics[\"total\"] += 1\n",
        "        if success: self.metrics[\"successful\"] += 1\n",
        "        else: self.metrics[\"failed\"] += 1\n",
        "        if fallback_used: self.metrics[\"fallback\"] += 1\n",
        "        if verified: self.metrics[\"verified\"] += 1\n",
        "\n",
        "# --- Global Instances ---\n",
        "symbolic_compute = SymbolicCompute()\n",
        "answer_validator = AnswerValidator()\n",
        "execution_metrics = ExecutionMetrics()\n",
        "inference_engine = AIOMInference(model_name=\"gpt2\") # Initialize inference\n",
        "\n",
        "try:\n",
        "    inference_engine.load_model()\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Inference engine load failed (expected in CI/CD without full weights): {e}\")\n",
        "\n",
        "# --- Prediction Logic ---\n",
        "\n",
        "predictions = []\n",
        "\n",
        "def solve_one_problem(problem_id, problem_text):\n",
        "    \"\"\"Solve a single problem and return the answer with verification.\"\"\"\n",
        "    try:\n",
        "        # 1. Generate initial answer\n",
        "        answer_str = inference_engine.generate(problem_text)\n",
        "        \n",
        "        # 2. Extract numeric candidate\n",
        "        initial_answer = 0\n",
        "        try:\n",
        "            initial_answer = int(float(answer_str))\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # 3. Apply Verification & Validation\n",
        "        # Symbolic verification\n",
        "        is_valid, confidence = symbolic_compute.verify_symbolic_result(initial_answer, str(problem_text))\n",
        "        \n",
        "        # Fallback validation if needed\n",
        "        if not is_valid:\n",
        "            validation_result = answer_validator.validate_with_fallback_strategies(initial_answer, str(problem_text))\n",
        "            final_answer = validation_result.get('final_answer', initial_answer)\n",
        "            fallback_used = True\n",
        "        else:\n",
        "            final_answer = initial_answer\n",
        "            fallback_used = False\n",
        "        \n",
        "        # 4. Final edge case check\n",
        "        final_answer, note = answer_validator.handle_edge_cases(final_answer)\n",
        "        \n",
        "        # 5. Record metrics\n",
        "        execution_metrics.record_result(success=True, verified=is_valid, fallback_used=fallback_used)\n",
        "        \n",
        "        return final_answer\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error on {problem_id}: {e}\")\n",
        "        return 0\n",
        "\n",
        "# Main Execution Loop\n",
        "if KAGGLE_MODE:\n",
        "    print(\"üöÄ Running in KAGGLE SUBMISSION MODE\")\n",
        "    for (test, sample_prediction) in iter_test:\n",
        "        test_row = test.iloc[0]\n",
        "        problem_id = str(test_row['id'])\n",
        "        problem_text = str(test_row['problem'])\n",
        "        \n",
        "        pred = solve_one_problem(problem_id, problem_text)\n",
        "        sample_prediction['answer'] = pred\n",
        "        env.predict(sample_prediction)\n",
        "        \n",
        "    print(\"‚úÖ Submission loop complete\")\n",
        "    \n",
        "else:\n",
        "    print(\"üîß Running in LOCAL DEBUG MODE\")\n",
        "    local_preds = []\n",
        "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
        "        problem_id = str(row['id'])\n",
        "        problem_text = str(row['problem'])\n",
        "        \n",
        "        pred = solve_one_problem(problem_id, problem_text)\n",
        "        local_preds.append({'id': problem_id, 'answer': pred})\n",
        "        \n",
        "    submission_df = pd.DataFrame(local_preds)\n",
        "    print(f\"‚úÖ Generated {len(submission_df)} local predictions\")\n"
    ]

    # 4. Update Save Submission (id: 62c03805)
    new_save_code = [
        "# 7. Save Submission (Only needed for local mode)\n",
        "if not KAGGLE_MODE:\n",
        "    submission_path = \"submission.csv\"\n",
        "    submission_df.to_csv(submission_path, index=False)\n",
        "    \n",
        "    print(f\"‚úÖ Submission saved to {submission_path}\")\n",
        "    print(f\"\\nFile size: {os.path.getsize(submission_path)} bytes\")\n",
        "    print(f\"Rows: {len(submission_df)}\")\n",
        "    print(f\"\\nFirst 5 rows:\")\n",
        "    print(submission_df.head())\n",
        "else:\n",
        "    print(\"‚úÖ Submission file generated via AIMO API\")\n"
    ]

    # 5. Update Imports for Phase 4 (id: f1276a94)
    new_import_code = [
        "# Add phase 4 source code to path using robust discovery\n",
        "import sys\n",
        "import os\n",
        "\n",
        "def find_source_dir(start_path='/kaggle/input', target_file='monitoring.py'):\n",
        "    \"\"\"Recursively find directory containing target file\"\"\"\n",
        "    # Check explicit paths first for speed\n",
        "    common_paths = [\n",
        "        '/kaggle/input/aimo-solver-phase4',\n",
        "        '/kaggle/input/datasets/muzansano/aimo-solver-phase4',\n",
        "        'src',\n",
        "        '../src'\n",
        "    ]\n",
        "    for p in common_paths:\n",
        "        if os.path.exists(os.path.join(p, target_file)):\n",
        "            return p\n",
        "\n",
        "    # Walk directory tree\n",
        "    for root, dirs, files in os.walk(start_path):\n",
        "        if target_file in files:\n",
        "            return root\n",
        "    return None\n",
        "\n",
        "src_path = find_source_dir()\n",
        "if src_path:\n",
        "    if src_path not in sys.path:\n",
        "        sys.path.insert(0, src_path)\n",
        "    print(f\"‚úÖ Phase 4 source found and added: {src_path}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Could not find 'monitoring.py'. Phase 4 features will be disabled.\")\n",
        "    # Debugging: List input directory structure\n",
        "    print(\"DEBUG: /kaggle/input structure:\")\n",
        "    try:\n",
        "        for root, dirs, files in os.walk('/kaggle/input'):\n",
        "            print(f\"{root} -> {dirs}\")\n",
        "    except: pass\n",
        "\n",
        "# Phase 4: Import Verification & Metrics Components\n",
        "try:\n",
        "    from monitoring import VerificationTracker, ExecutionMetrics\n",
        "    from resilience import ErrorRecoveryHandler\n",
        "    from computation import SymbolicCompute, AnswerValidator\n",
        "    \n",
        "    PHASE4_AVAILABLE = True\n",
        "    print(\"‚úÖ Phase 4 components imported successfully\")\n",
        "except ImportError as e:\n",
        "    PHASE4_AVAILABLE = False\n",
        "    print(f\"‚ö†Ô∏è Phase 4 components not found: {e}\")\n",
        "    print(\"   Verification features will be disabled.\")\n"
    ]

    # Apply updates
    updates_made = 0
    for cell in nb["cells"]:
        cell_id = cell.get("id")
        if cell_id == "a4d3f7b9":
            cell["source"] = new_inference_code
            updates_made += 1
            print("Updated AIOMInference class")
        elif cell_id == "5365a938":
            cell["source"] = new_data_loading_code
            updates_made += 1
            print("Updated Data Loading")
        elif cell_id == "78aa4489":
            cell["source"] = new_prediction_code
            updates_made += 1
            print("Updated Prediction Loop")
        elif cell_id == "62c03805":
            cell["source"] = new_save_code
            updates_made += 1
            print("Updated Save Submission")
        elif cell_id == "f1276a94":
            cell["source"] = new_import_code
            updates_made += 1
            print("Updated Imports")

    if updates_made >= 4:
        with open(NOTEBOOK_PATH, 'w') as f:
            json.dump(nb, f, indent=1)
        print("SUCCESS: Notebook updated successfully.")
    else:
        print(f"WARNING: Only updated {updates_made}/4 cells. Check cell IDs.")

if __name__ == "__main__":
    update_notebook()
