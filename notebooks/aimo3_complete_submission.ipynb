{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# AIMO3 Complete Submission - All Phases (1-4)\n",
                "## AI Mathematical Olympiad Progress Prize 3\n",
                "\n",
                "**Features:**\n",
                "- Phase 1-3: LLM reasoning with chain-of-thought\n",
                "- Phase 4: Symbolic verification + fallback validation + edge case handling\n",
                "- Competition API integration (aimo)\n",
                "- Comprehensive metrics tracking\n",
                "\n",
                "**Answer Format:** Integer (0-99,999)\n",
                "**Evaluation:** Penalized accuracy (2 runs per problem)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "install_deps",
            "metadata": {},
            "source": [
                "## 1. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "dependencies",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install --quiet sympy torch transformers peft pandas tqdm"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "imports_header",
            "metadata": {},
            "source": [
                "## 2. Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import re\n",
                "import json\n",
                "import logging\n",
                "from typing import Dict, List, Any, Optional, Union, Tuple\n",
                "from datetime import datetime\n",
                "\n",
                "import torch\n",
                "import pandas as pd\n",
                "import sympy as sp\n",
                "from tqdm import tqdm\n",
                "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
                "\n",
                "# Setup logging\n",
                "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
                "logger = logging.getLogger(__name__)\n",
                "\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
                "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
                "print(f\"Using device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "phase4_header",
            "metadata": {},
            "source": [
                "## 3. Phase 4: Computation Components (Inline)\n",
                "\n",
                "These classes provide symbolic verification, fallback validation, and metrics tracking."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "phase4_symbolic",
            "metadata": {},
            "outputs": [],
            "source": [
                "class SymbolicCompute:\n",
                "    \"\"\"SymPy-based symbolic computation and verification.\"\"\"\n",
                "    \n",
                "    @staticmethod\n",
                "    def evaluate_expression(expr_str: str) -> Optional[Union[int, float]]:\n",
                "        \"\"\"Evaluate a mathematical expression string using SymPy.\"\"\"\n",
                "        try:\n",
                "            expr = sp.sympify(expr_str)\n",
                "            result = expr.evalf()\n",
                "            return int(result) if result == int(result) else float(result)\n",
                "        except Exception as e:\n",
                "            logger.debug(f\"Failed to evaluate expression '{expr_str}': {str(e)}\")\n",
                "            return None\n",
                "    \n",
                "    @staticmethod\n",
                "    def parse_llm_output_for_expressions(llm_text: str) -> Dict[str, Any]:\n",
                "        \"\"\"Parse LLM output to extract mathematical expressions.\"\"\"\n",
                "        result = {\n",
                "            \"expressions\": [],\n",
                "            \"values\": [],\n",
                "            \"final_value\": None\n",
                "        }\n",
                "        \n",
                "        try:\n",
                "            # Extract expressions\n",
                "            expr_pattern = r\"(?:=|equals|is)\\s*(\\d+(?:\\.\\d+)?|[\\w\\s\\+\\-\\*/\\(\\)\\.]+)\"\n",
                "            expr_matches = re.findall(expr_pattern, llm_text, re.IGNORECASE)\n",
                "            \n",
                "            # Extract numeric values\n",
                "            num_pattern = r\"\\b(\\d+(?:\\.\\d+)?)\\b\"\n",
                "            num_matches = re.findall(num_pattern, llm_text)\n",
                "            \n",
                "            result[\"expressions\"] = expr_matches[:5]\n",
                "            result[\"values\"] = [float(n) if '.' in n else int(n) for n in num_matches[:10]]\n",
                "            \n",
                "            if num_matches:\n",
                "                result[\"final_value\"] = float(num_matches[-1]) if '.' in num_matches[-1] else int(num_matches[-1])\n",
                "                \n",
                "        except Exception as e:\n",
                "            logger.debug(f\"Failed to parse LLM output: {str(e)}\")\n",
                "        \n",
                "        return result\n",
                "    \n",
                "    @staticmethod\n",
                "    def verify_symbolic_result(\n",
                "        llm_answer: int,\n",
                "        llm_output: str,\n",
                "        tolerance: float = 0.01\n",
                "    ) -> Tuple[bool, float]:\n",
                "        \"\"\"Verify LLM answer by symbolic computation. Returns (is_valid, confidence).\"\"\"\n",
                "        try:\n",
                "            parsed = SymbolicCompute.parse_llm_output_for_expressions(llm_output)\n",
                "            \n",
                "            # Try to evaluate extracted expressions\n",
                "            if parsed[\"expressions\"]:\n",
                "                for expr_str in parsed[\"expressions\"]:\n",
                "                    try:\n",
                "                        result = SymbolicCompute.evaluate_expression(expr_str)\n",
                "                        if result is not None:\n",
                "                            if isinstance(result, float):\n",
                "                                diff_percent = abs(result - llm_answer) / max(abs(llm_answer), 1)\n",
                "                                if diff_percent <= tolerance:\n",
                "                                    return True, 1.0 - diff_percent\n",
                "                            else:\n",
                "                                if int(result) == llm_answer:\n",
                "                                    return True, 1.0\n",
                "                    except:\n",
                "                        continue\n",
                "            \n",
                "            # Check final value\n",
                "            if parsed[\"final_value\"] is not None and parsed[\"final_value\"] == llm_answer:\n",
                "                return True, 0.8\n",
                "                    \n",
                "        except Exception as e:\n",
                "            logger.debug(f\"Verification failed: {str(e)}\")\n",
                "        \n",
                "        return False, 0.5\n",
                "\n",
                "print(\"‚úÖ SymbolicCompute class loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "phase4_validator",
            "metadata": {},
            "outputs": [],
            "source": [
                "class AnswerValidator:\n",
                "    \"\"\"Validates and enforces answer format constraints.\"\"\"\n",
                "    \n",
                "    AIMO_MIN = 0\n",
                "    AIMO_MAX = 99999\n",
                "    \n",
                "    @staticmethod\n",
                "    def validate_integer(value: Any) -> Optional[int]:\n",
                "        \"\"\"Validate and convert value to valid AIMO integer.\"\"\"\n",
                "        try:\n",
                "            int_value = int(float(str(value).strip()))\n",
                "            \n",
                "            if int_value < AnswerValidator.AIMO_MIN:\n",
                "                return AnswerValidator.AIMO_MIN\n",
                "            \n",
                "            if int_value > AnswerValidator.AIMO_MAX:\n",
                "                return AnswerValidator.AIMO_MAX\n",
                "            \n",
                "            return int_value\n",
                "        except (ValueError, TypeError) as e:\n",
                "            logger.debug(f\"Failed to validate answer: {str(e)}\")\n",
                "            return None\n",
                "    \n",
                "    @staticmethod\n",
                "    def extract_and_validate_answer(text: str) -> Optional[int]:\n",
                "        \"\"\"Extract numeric answer from text and validate it.\"\"\"\n",
                "        patterns = [\n",
                "            r\"(?:answer|result|final answer)\\s*:?\\s*(\\d+)\",\n",
                "            r\"(?:the answer is|equals)\\s*(\\d+)\",\n",
                "            r\"(\\d{1,5})\\s*(?:is the answer|is correct)\"\n",
                "        ]\n",
                "        \n",
                "        for pattern in patterns:\n",
                "            match = re.search(pattern, text, re.IGNORECASE)\n",
                "            if match:\n",
                "                try:\n",
                "                    candidate = int(match.group(1))\n",
                "                    validated = AnswerValidator.validate_integer(candidate)\n",
                "                    if validated is not None:\n",
                "                        return validated\n",
                "                except ValueError:\n",
                "                    continue\n",
                "        \n",
                "        # Fallback: find all numbers and validate the last one\n",
                "        numbers = re.findall(r\"\\d+\", text)\n",
                "        if numbers:\n",
                "            return AnswerValidator.validate_integer(numbers[-1])\n",
                "        \n",
                "        return None\n",
                "    \n",
                "    @staticmethod\n",
                "    def validate_with_fallback_strategies(\n",
                "        llm_answer: Optional[int],\n",
                "        llm_text: str\n",
                "    ) -> Dict[str, Any]:\n",
                "        \"\"\"Validate answer with multiple fallback strategies.\"\"\"\n",
                "        result = {\n",
                "            \"final_answer\": 0,\n",
                "            \"confidence\": 0.0,\n",
                "            \"strategy_used\": \"default_fallback\",\n",
                "            \"fallback_applied\": False\n",
                "        }\n",
                "        \n",
                "        try:\n",
                "            # Strategy 1: Use primary answer if valid\n",
                "            if llm_answer is not None:\n",
                "                validated = AnswerValidator.validate_integer(llm_answer)\n",
                "                if validated is not None:\n",
                "                    result[\"final_answer\"] = validated\n",
                "                    result[\"confidence\"] = 0.9\n",
                "                    result[\"strategy_used\"] = \"primary_llm_answer\"\n",
                "                    return result\n",
                "            \n",
                "            # Strategy 2: Try symbolic verification\n",
                "            is_valid, confidence = SymbolicCompute.verify_symbolic_result(\n",
                "                llm_answer if llm_answer is not None else 0,\n",
                "                llm_text\n",
                "            )\n",
                "            \n",
                "            if is_valid and llm_answer is not None:\n",
                "                validated = AnswerValidator.validate_integer(llm_answer)\n",
                "                if validated is not None:\n",
                "                    result[\"final_answer\"] = validated\n",
                "                    result[\"confidence\"] = confidence\n",
                "                    result[\"strategy_used\"] = \"symbolic_verification\"\n",
                "                    result[\"fallback_applied\"] = True\n",
                "                    return result\n",
                "            \n",
                "            # Strategy 3: Re-extract from text\n",
                "            extracted = AnswerValidator.extract_and_validate_answer(llm_text)\n",
                "            if extracted is not None:\n",
                "                result[\"final_answer\"] = extracted\n",
                "                result[\"confidence\"] = 0.75\n",
                "                result[\"strategy_used\"] = \"text_reextraction\"\n",
                "                result[\"fallback_applied\"] = True\n",
                "                return result\n",
                "            \n",
                "            # Strategy 4: Default fallback\n",
                "            result[\"fallback_applied\"] = True\n",
                "            \n",
                "        except Exception as e:\n",
                "            logger.debug(f\"Fallback validation failed: {str(e)}\")\n",
                "        \n",
                "        return result\n",
                "    \n",
                "    @staticmethod\n",
                "    def handle_edge_cases(answer: int, problem_context: str = \"\") -> Tuple[int, str]:\n",
                "        \"\"\"Handle edge cases in answer validation.\"\"\"\n",
                "        edge_case_note = \"\"\n",
                "        \n",
                "        try:\n",
                "            # Negative answers\n",
                "            if answer < 0:\n",
                "                edge_case_note = f\"Negative answer {answer} converted to 0\"\n",
                "                return 0, edge_case_note\n",
                "            \n",
                "            # Very large answers\n",
                "            if answer > AnswerValidator.AIMO_MAX * 10:\n",
                "                if \"mod\" in problem_context.lower():\n",
                "                    modulus = 1000\n",
                "                    answer = answer % modulus\n",
                "                    edge_case_note = f\"Large answer modded to {answer}\"\n",
                "                    return answer, edge_case_note\n",
                "                else:\n",
                "                    answer = AnswerValidator.AIMO_MAX\n",
                "                    edge_case_note = f\"Very large answer capped at {AnswerValidator.AIMO_MAX}\"\n",
                "                    return answer, edge_case_note\n",
                "            \n",
                "            validated = AnswerValidator.validate_integer(answer)\n",
                "            return validated if validated is not None else 0, edge_case_note\n",
                "            \n",
                "        except Exception as e:\n",
                "            logger.debug(f\"Edge case handling failed: {str(e)}\")\n",
                "            return 0, f\"Edge case error: {str(e)}\"\n",
                "\n",
                "print(\"‚úÖ AnswerValidator class loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "phase4_metrics",
            "metadata": {},
            "outputs": [],
            "source": [
                "class ExecutionMetrics:\n",
                "    \"\"\"Track execution metrics for the pipeline.\"\"\"\n",
                "    \n",
                "    def __init__(self):\n",
                "        self.metrics = {\n",
                "            \"total_processed\": 0,\n",
                "            \"successful\": 0,\n",
                "            \"failed\": 0,\n",
                "            \"fallback_used\": 0,\n",
                "            \"verified\": 0,\n",
                "            \"average_confidence\": 0.0\n",
                "        }\n",
                "    \n",
                "    def record_result(\n",
                "        self,\n",
                "        success: bool,\n",
                "        fallback_used: bool = False,\n",
                "        verified: bool = False,\n",
                "        confidence: float = 0.0\n",
                "    ) -> None:\n",
                "        \"\"\"Record a single result.\"\"\"\n",
                "        self.metrics[\"total_processed\"] += 1\n",
                "        \n",
                "        if success:\n",
                "            self.metrics[\"successful\"] += 1\n",
                "        else:\n",
                "            self.metrics[\"failed\"] += 1\n",
                "        \n",
                "        if fallback_used:\n",
                "            self.metrics[\"fallback_used\"] += 1\n",
                "        \n",
                "        if verified:\n",
                "            self.metrics[\"verified\"] += 1\n",
                "        \n",
                "        # Update average confidence\n",
                "        if self.metrics[\"successful\"] > 0:\n",
                "            self.metrics[\"average_confidence\"] = (\n",
                "                (self.metrics[\"average_confidence\"] * (self.metrics[\"successful\"] - 1) + confidence) /\n",
                "                self.metrics[\"successful\"]\n",
                "            )\n",
                "    \n",
                "    def get_summary(self) -> Dict[str, Any]:\n",
                "        \"\"\"Get execution summary.\"\"\"\n",
                "        total = self.metrics[\"total_processed\"]\n",
                "        \n",
                "        return {\n",
                "            \"total_processed\": total,\n",
                "            \"successful\": self.metrics[\"successful\"],\n",
                "            \"failed\": self.metrics[\"failed\"],\n",
                "            \"success_rate\": self.metrics[\"successful\"] / total if total > 0 else 0.0,\n",
                "            \"fallback_used_count\": self.metrics[\"fallback_used\"],\n",
                "            \"fallback_rate\": self.metrics[\"fallback_used\"] / total if total > 0 else 0.0,\n",
                "            \"verified_count\": self.metrics[\"verified\"],\n",
                "            \"verification_rate\": self.metrics[\"verified\"] / total if total > 0 else 0.0,\n",
                "            \"average_confidence\": self.metrics[\"average_confidence\"]\n",
                "        }\n",
                "\n",
                "print(\"‚úÖ ExecutionMetrics class loaded\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model_header",
            "metadata": {},
            "source": [
                "## 4. Load LLM Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "model_config",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model configuration - Change this to use stronger models on Kaggle\n",
                "MODEL_NAME = \"gpt2\"  # Fast baseline for testing\n",
                "# For better performance, use: \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\" or similar\n",
                "\n",
                "MAX_TOKENS = 512\n",
                "TEMPERATURE = 0.7\n",
                "\n",
                "print(f\"Loading model: {MODEL_NAME}\")\n",
                "print(f\"Using device: {DEVICE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "model_load",
            "metadata": {},
            "outputs": [],
            "source": [
                "try:\n",
                "    # Load tokenizer and model\n",
                "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "    model = AutoModelForCausalLM.from_pretrained(\n",
                "        MODEL_NAME,\n",
                "        torch_dtype=torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
                "    )\n",
                "    \n",
                "    if DEVICE == \"cuda\":\n",
                "        model = model.to(DEVICE)\n",
                "    \n",
                "    model.eval()\n",
                "    \n",
                "    if tokenizer.pad_token is None:\n",
                "        tokenizer.pad_token = tokenizer.eos_token\n",
                "    \n",
                "    print(\"‚úÖ Model loaded successfully!\")\n",
                "    MODEL_AVAILABLE = True\n",
                "except Exception as e:\n",
                "    print(f\"‚ö†Ô∏è Model loading failed: {e}\")\n",
                "    print(\"Continuing with preprocessing-only mode...\")\n",
                "    MODEL_AVAILABLE = False"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "functions_header",
            "metadata": {},
            "source": [
                "## 5. Define Helper Functions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "preprocessing",
            "metadata": {},
            "outputs": [],
            "source": [
                "def latex_to_text(latex_expr: str) -> str:\n",
                "    \"\"\"Convert LaTeX expressions to plain text.\"\"\"\n",
                "    text = re.sub(r\"\\\\\\\\\", \"\", latex_expr)\n",
                "    text = re.sub(r\"\\$\\$|\\$\", \"\", text)\n",
                "    text = re.sub(r\"\\\\left|\\\\right\", \"\", text)\n",
                "    text = re.sub(r\"\\\\begin\\{.*?\\}|\\\\end\\{.*?\\}\", \"\", text)\n",
                "    text = re.sub(r\"\\\\text\\{\", \"\", text)\n",
                "    text = re.sub(r\"\\}\", \"\", text)\n",
                "    text = re.sub(r\"\\\\frac\", \"frac\", text)\n",
                "    text = re.sub(r\"\\\\sqrt\", \"sqrt\", text)\n",
                "    text = re.sub(r\"\\\\[a-z]+\", \"\", text)\n",
                "    text = re.sub(r\"\\s+\", \" \", text)\n",
                "    return text.strip()\n",
                "\n",
                "def prepare_problem(input_data: str) -> str:\n",
                "    \"\"\"Prepare problem for LLM input.\"\"\"\n",
                "    return latex_to_text(str(input_data))\n",
                "\n",
                "print(\"‚úÖ Preprocessing functions defined\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "llm_functions",
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_prompt(problem_text: str) -> str:\n",
                "    \"\"\"Create chain-of-thought prompt for LLM.\"\"\"\n",
                "    return f\"\"\"You are a mathematical expert solving Olympiad-level problems.\n",
                "Solve the following problem step-by-step:\n",
                "\n",
                "Problem: {problem_text}\n",
                "\n",
                "Solution:\n",
                "Let me work through this carefully.\n",
                "\n",
                "Step 1: \"\"\"\n",
                "\n",
                "def llm_solve(problem_text: str) -> Dict[str, Any]:\n",
                "    \"\"\"Solve problem using LLM with chain-of-thought reasoning.\"\"\"\n",
                "    if not MODEL_AVAILABLE:\n",
                "        return {\"problem\": problem_text, \"reasoning\": None, \"error\": \"Model not available\"}\n",
                "    \n",
                "    try:\n",
                "        prompt = create_prompt(problem_text)\n",
                "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
                "        \n",
                "        with torch.no_grad():\n",
                "            outputs = model.generate(\n",
                "                **inputs,\n",
                "                max_new_tokens=MAX_TOKENS,\n",
                "                temperature=TEMPERATURE,\n",
                "                top_p=0.9,\n",
                "                do_sample=True,\n",
                "                pad_token_id=tokenizer.eos_token_id\n",
                "            )\n",
                "        \n",
                "        raw_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "        \n",
                "        # Extract reasoning\n",
                "        if \"Solution:\" in raw_output:\n",
                "            reasoning = raw_output.split(\"Solution:\")[-1]\n",
                "        else:\n",
                "            reasoning = raw_output\n",
                "        \n",
                "        return {\n",
                "            \"problem\": problem_text,\n",
                "            \"reasoning\": reasoning.strip(),\n",
                "            \"raw_output\": raw_output\n",
                "        }\n",
                "    except Exception as e:\n",
                "        return {\"problem\": problem_text, \"error\": str(e), \"reasoning\": None}\n",
                "\n",
                "def extract_numeric_answer(text: str) -> Optional[int]:\n",
                "    \"\"\"Extract numeric answer from LLM output.\"\"\"\n",
                "    if text is None:\n",
                "        return None\n",
                "    \n",
                "    patterns = [\n",
                "        r\"(?:answer|result|final answer)\\s*:?\\s*(\\d+)\",\n",
                "        r\"(?:the answer is|equals)\\s*(\\d+)\",\n",
                "        r\"(\\d{1,5})\\s*(?:is the answer|is correct)\"\n",
                "    ]\n",
                "    \n",
                "    for pattern in patterns:\n",
                "        match = re.search(pattern, text, re.IGNORECASE)\n",
                "        if match:\n",
                "            try:\n",
                "                return int(match.group(1))\n",
                "            except ValueError:\n",
                "                continue\n",
                "    \n",
                "    # Fallback: find all numbers and return the last one\n",
                "    numbers = re.findall(r\"\\d+\", text)\n",
                "    if numbers:\n",
                "        return int(numbers[-1])\n",
                "    \n",
                "    return None\n",
                "\n",
                "print(\"‚úÖ LLM functions defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "solve_header",
            "metadata": {},
            "source": [
                "## 6. Phase 4 Integrated Solver Function"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "integrated_solver",
            "metadata": {},
            "outputs": [],
            "source": [
                "def solve_with_phase4_verification(\n",
                "    problem_text: str,\n",
                "    problem_id: str,\n",
                "    metrics_tracker: ExecutionMetrics\n",
                ") -> Dict[str, Any]:\n",
                "    \"\"\"\n",
                "    Solve problem with full Phase 4 verification workflow.\n",
                "    \n",
                "    Workflow:\n",
                "    1. Preprocess problem\n",
                "    2. LLM reasoning\n",
                "    3. Extract answer\n",
                "    4. Phase 4: Symbolic verification\n",
                "    5. Phase 4: Fallback validation\n",
                "    6. Phase 4: Edge case handling\n",
                "    7. Metrics tracking\n",
                "    \"\"\"\n",
                "    result = {\n",
                "        \"problem_id\": problem_id,\n",
                "        \"problem_text\": problem_text,\n",
                "        \"final_answer\": 0,\n",
                "        \"confidence\": 0.0,\n",
                "        \"strategy_used\": \"default\",\n",
                "        \"verified\": False,\n",
                "        \"error\": None\n",
                "    }\n",
                "    \n",
                "    try:\n",
                "        # Step 1: Preprocess\n",
                "        prepared_problem = prepare_problem(problem_text)\n",
                "        \n",
                "        # Step 2-3: LLM Reasoning + Answer Extraction\n",
                "        llm_result = llm_solve(prepared_problem)\n",
                "        \n",
                "        if \"error\" in llm_result:\n",
                "            result[\"error\"] = llm_result[\"error\"]\n",
                "            metrics_tracker.record_result(success=False)\n",
                "            return result\n",
                "        \n",
                "        llm_answer = extract_numeric_answer(llm_result[\"reasoning\"])\n",
                "        \n",
                "        # Step 4: Symbolic Verification\n",
                "        is_verified, verification_confidence = SymbolicCompute.verify_symbolic_result(\n",
                "            llm_answer if llm_answer is not None else 0,\n",
                "            llm_result[\"reasoning\"]\n",
                "        )\n",
                "        \n",
                "        # Step 5: Fallback Validation\n",
                "        validation_result = AnswerValidator.validate_with_fallback_strategies(\n",
                "            llm_answer,\n",
                "            llm_result[\"reasoning\"]\n",
                "        )\n",
                "        \n",
                "        # Step 6: Edge Case Handling\n",
                "        final_answer, edge_case_note = AnswerValidator.handle_edge_cases(\n",
                "            validation_result[\"final_answer\"],\n",
                "            problem_text\n",
                "        )\n",
                "        \n",
                "        # Update result\n",
                "        result[\"final_answer\"] = final_answer\n",
                "        result[\"confidence\"] = verification_confidence if is_verified else validation_result[\"confidence\"]\n",
                "        result[\"strategy_used\"] = validation_result[\"strategy_used\"]\n",
                "        result[\"verified\"] = is_verified\n",
                "        result[\"fallback_applied\"] = validation_result[\"fallback_applied\"]\n",
                "        result[\"edge_case_note\"] = edge_case_note\n",
                "        \n",
                "        # Step 7: Track Metrics\n",
                "        metrics_tracker.record_result(\n",
                "            success=True,\n",
                "            fallback_used=validation_result[\"fallback_applied\"],\n",
                "            verified=is_verified,\n",
                "            confidence=result[\"confidence\"]\n",
                "        )\n",
                "        \n",
                "    except Exception as e:\n",
                "        logger.error(f\"Error solving {problem_id}: {str(e)}\")\n",
                "        result[\"error\"] = str(e)\n",
                "        metrics_tracker.record_result(success=False)\n",
                "    \n",
                "    return result\n",
                "\n",
                "print(\"‚úÖ Integrated solver function defined\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "test_header",
            "metadata": {},
            "source": [
                "## 7. Test on Sample Problems"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "sample_test",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialize metrics tracker\n",
                "metrics_tracker = ExecutionMetrics()\n",
                "\n",
                "# Sample problems for testing\n",
                "sample_problems = [\n",
                "    {\"id\": \"test_001\", \"problem\": \"What is $2 + 3 \\\\times 5$?\"},\n",
                "    {\"id\": \"test_002\", \"problem\": \"Solve $2x + 5 = 13$. What is $x$?\"},\n",
                "    {\"id\": \"test_003\", \"problem\": \"Find $7 \\\\times 8$.\"}\n",
                "]\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"TESTING PHASE 4 INTEGRATED SOLVER\")\n",
                "print(\"=\"*60 + \"\\n\")\n",
                "\n",
                "test_results = []\n",
                "for prob in sample_problems:\n",
                "    result = solve_with_phase4_verification(\n",
                "        problem_text=prob[\"problem\"],\n",
                "        problem_id=prob[\"id\"],\n",
                "        metrics_tracker=metrics_tracker\n",
                "    )\n",
                "    test_results.append(result)\n",
                "    \n",
                "    print(f\"Problem {prob['id']}: {prob['problem']}\")\n",
                "    print(f\"  Answer: {result['final_answer']}\")\n",
                "    print(f\"  Confidence: {result['confidence']:.2f}\")\n",
                "    print(f\"  Strategy: {result['strategy_used']}\")\n",
                "    print(f\"  Verified: {result['verified']}\")\n",
                "    print()\n",
                "\n",
                "# Display metrics summary\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"PHASE 4 METRICS SUMMARY\")\n",
                "print(\"=\"*60)\n",
                "summary = metrics_tracker.get_summary()\n",
                "for key, value in summary.items():\n",
                "    if 'rate' in key or 'confidence' in key:\n",
                "        print(f\"{key}: {value:.1%}\" if 'rate' in key else f\"{key}: {value:.2f}\")\n",
                "    else:\n",
                "        print(f\"{key}: {value}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "api_header",
            "metadata": {},
            "source": [
                "## 8. Competition API Integration\n",
                "\n",
                "**Note:** On Kaggle, you'll use the `aimo` API to iterate through problems.\n",
                "For local testing, we'll simulate this with a CSV file."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "load_test_data",
            "metadata": {},
            "outputs": [],
            "source": [
                "# For Kaggle submission, uncomment and use this:\n",
                "# from aimo import Inference\n",
                "# inference = Inference()\n",
                "\n",
                "# For local testing, load from CSV\n",
                "test_data_paths = [\n",
                "    \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv\",  # Kaggle path\n",
                "    \"../datasets/aimo3_test.csv\",  # Local path\n",
                "]\n",
                "\n",
                "test_df = None\n",
                "for path in test_data_paths:\n",
                "    if os.path.exists(path):\n",
                "        test_df = pd.read_csv(path)\n",
                "        print(f\"‚úÖ Loaded test data from {path}\")\n",
                "        print(f\"   Total problems: {len(test_df)}\")\n",
                "        break\n",
                "\n",
                "if test_df is None:\n",
                "    print(\"‚ö†Ô∏è No test data found. Using sample problems.\")\n",
                "    test_df = pd.DataFrame(sample_problems)\n",
                "    test_df.columns = ['id', 'problem']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "solve_all",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Solve all problems with Phase 4 verification\n",
                "print(f\"\\nSolving {len(test_df)} problems with Phase 4 verification...\\n\")\n",
                "\n",
                "predictions = []\n",
                "detailed_results = []\n",
                "\n",
                "# Reset metrics for full run\n",
                "full_metrics = ExecutionMetrics()\n",
                "\n",
                "for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Solving\"):\n",
                "    problem_id = str(row['id'])\n",
                "    problem_text = str(row['problem'])\n",
                "    \n",
                "    result = solve_with_phase4_verification(\n",
                "        problem_text=problem_text,\n",
                "        problem_id=problem_id,\n",
                "        metrics_tracker=full_metrics\n",
                "    )\n",
                "    \n",
                "    predictions.append({\n",
                "        'id': problem_id,\n",
                "        'answer': result['final_answer']\n",
                "    })\n",
                "    \n",
                "    detailed_results.append(result)\n",
                "\n",
                "print(f\"\\n‚úÖ Solved {len(predictions)} problems\")\n",
                "\n",
                "# Create submission dataframe\n",
                "submission_df = pd.DataFrame(predictions)\n",
                "print(f\"\\nSubmission Preview:\")\n",
                "print(submission_df.head(10))"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "metrics_header",
            "metadata": {},
            "source": [
                "## 9. Phase 4 Metrics Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "metrics_report",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"PHASE 4 EXECUTION METRICS REPORT\")\n",
                "print(\"=\"*80 + \"\\n\")\n",
                "\n",
                "final_summary = full_metrics.get_summary()\n",
                "\n",
                "print(\"üìä Overall Statistics:\")\n",
                "print(f\"  Total Processed: {final_summary['total_processed']}\")\n",
                "print(f\"  Successful: {final_summary['successful']}\")\n",
                "print(f\"  Failed: {final_summary['failed']}\")\n",
                "print(f\"  Success Rate: {final_summary['success_rate']:.1%}\")\n",
                "print()\n",
                "\n",
                "print(\"üîç Verification Metrics:\")\n",
                "print(f\"  Verified Count: {final_summary['verified_count']}\")\n",
                "print(f\"  Verification Rate: {final_summary['verification_rate']:.1%}\")\n",
                "print(f\"  Average Confidence: {final_summary['average_confidence']:.2f}\")\n",
                "print()\n",
                "\n",
                "print(\"üîÑ Fallback Statistics:\")\n",
                "print(f\"  Fallback Used: {final_summary['fallback_used_count']}\")\n",
                "print(f\"  Fallback Rate: {final_summary['fallback_rate']:.1%}\")\n",
                "print()\n",
                "\n",
                "# Answer statistics\n",
                "answers = submission_df['answer'].tolist()\n",
                "print(\"üìà Answer Statistics:\")\n",
                "print(f\"  Min Answer: {min(answers)}\")\n",
                "print(f\"  Max Answer: {max(answers)}\")\n",
                "print(f\"  Mean Answer: {sum(answers)/len(answers):.2f}\")\n",
                "print(f\"  Valid Range (0-99999): {all(0 <= a <= 99999 for a in answers)}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "save_header",
            "metadata": {},
            "source": [
                "## 10. Save Submission"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save_submission",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save submission CSV\n",
                "submission_path = \"submission.csv\"\n",
                "submission_df.to_csv(submission_path, index=False)\n",
                "print(f\"‚úÖ Submission saved to {submission_path}\")\n",
                "\n",
                "# Save detailed results with Phase 4 metadata\n",
                "detailed_path = \"detailed_results_phase4.json\"\n",
                "with open(detailed_path, 'w') as f:\n",
                "    json.dump(detailed_results, f, indent=2)\n",
                "print(f\"‚úÖ Detailed results saved to {detailed_path}\")\n",
                "\n",
                "# Save metrics summary\n",
                "metrics_path = \"phase4_metrics.json\"\n",
                "with open(metrics_path, 'w') as f:\n",
                "    json.dump({\n",
                "        \"summary\": final_summary,\n",
                "        \"timestamp\": datetime.now().isoformat()\n",
                "    }, f, indent=2)\n",
                "print(f\"‚úÖ Metrics saved to {metrics_path}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"‚úÖ SUBMISSION COMPLETE - ALL PHASES (1-4) EXECUTED\")\n",
                "print(\"=\"*80)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}