{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "360974c9",
   "metadata": {},
   "source": [
    "# AIMO3 Kaggle Submission - Multi-Model Support\n",
    "## Fine-tuned LLM Solver with Model Selection\n",
    "\n",
    "This notebook generates predictions for the AIMO3 competition using selectable models:\n",
    "- **Fast Inference**: GPT-2, Gemma 3 (4B/12B)\n",
    "- **Strong Reasoning**: Llama 4, Qwen 3, DeepSeek-R1, Mistral Large 3\n",
    "- **Ensemble**: Combine multiple models for better accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe962059",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad2ea39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install --quiet torch transformers peft pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86507f56",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Configure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114fe796",
   "metadata": {},
   "source": [
    "## Phase 4: Import Verification & Metrics Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1276a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add phase 4 source code to path using robust discovery\n",
    "import sys\n",
    "import os\n",
    "\n",
    "def find_source_dir(start_path='/kaggle/input', target_file='monitoring.py'):\n",
    "    \"\"\"Recursively find directory containing target file\"\"\"\n",
    "    # Check explicit paths first for speed\n",
    "    common_paths = [\n",
    "        '/kaggle/input/aimo-solver-phase4',\n",
    "        '/kaggle/input/datasets/muzansano/aimo-solver-phase4',\n",
    "        'src',\n",
    "        '../src'\n",
    "    ]\n",
    "    for p in common_paths:\n",
    "        if os.path.exists(os.path.join(p, target_file)):\n",
    "            return p\n",
    "\n",
    "    # Walk directory tree\n",
    "    for root, dirs, files in os.walk(start_path):\n",
    "        if target_file in files:\n",
    "            return root\n",
    "    return None\n",
    "\n",
    "src_path = find_source_dir()\n",
    "if src_path:\n",
    "    if src_path not in sys.path:\n",
    "        sys.path.insert(0, src_path)\n",
    "    print(f\"\u2705 Phase 4 source found and added: {src_path}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f Could not find 'monitoring.py'. Phase 4 features will be disabled.\")\n",
    "    # Debugging: List input directory structure\n",
    "    print(\"DEBUG: /kaggle/input structure:\")\n",
    "    try:\n",
    "        for root, dirs, files in os.walk('/kaggle/input'):\n",
    "            print(f\"{root} -> {dirs}\")\n",
    "    except: pass\n",
    "\n",
    "# Phase 4: Import Verification & Metrics Components\n",
    "try:\n",
    "    from monitoring import VerificationTracker, ExecutionMetrics\n",
    "    from resilience import ErrorRecoveryHandler\n",
    "    from computation import SymbolicCompute, AnswerValidator\n",
    "    \n",
    "    PHASE4_AVAILABLE = True\n",
    "    print(\"\u2705 Phase 4 components imported successfully\")\n",
    "except ImportError as e:\n",
    "    PHASE4_AVAILABLE = False\n",
    "    print(f\"\u26a0\ufe0f Phase 4 components not found: {e}\")\n",
    "    print(\"   Verification features will be disabled.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18edf5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: Initialize verification components if available\n",
    "if PHASE4_AVAILABLE:\n",
    "    verification_tracker = VerificationTracker()\n",
    "    error_recovery = ErrorRecoveryHandler()\n",
    "    execution_metrics = ExecutionMetrics()\n",
    "    symbolic_compute = SymbolicCompute()\n",
    "    answer_validator = AnswerValidator()\n",
    "    \n",
    "    print(\"\u2705 Phase 4 verification system initialized\")\n",
    "    print(f\"   - Verification Tracker ready\")\n",
    "    print(f\"   - Error Recovery Handler ready\")\n",
    "    print(f\"   - Execution Metrics ready\")\n",
    "else:\n",
    "    verification_tracker = None\n",
    "    error_recovery = None\n",
    "    execution_metrics = None\n",
    "    print(\"\u26a0\ufe0f Phase 4 verification disabled - using basic predictions only\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b81ca423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hssn/Documents/kaggle/ai|mo/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.10.0+cu128\n",
      "CUDA Available: False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Optional\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0403c",
   "metadata": {},
   "source": [
    "## 3. Define Model Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d3f7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Inference class defined\n"
     ]
    }
   ],
   "source": [
    "class AIOMInference:\n",
    "    \"\"\"Multi-model inference engine with model selection\"\"\"\n",
    "    \n",
    "    # Supported models\n",
    "    SUPPORTED_MODELS = {\n",
    "        \"gpt2\": {\"name\": \"GPT-2 (Fast Baseline)\", \"vram\": 1, \"speed\": \"instant\"},\n",
    "        \"gemma3-4b\": {\"name\": \"Gemma 3 4B\", \"vram\": 8, \"speed\": \"3s/q\"},\n",
    "        \"gemma3-12b\": {\"name\": \"Gemma 3 12B\", \"vram\": 24, \"speed\": \"6s/q\"},\n",
    "        \"llama4-scout\": {\"name\": \"Llama 4 Scout 8B\", \"vram\": 16, \"speed\": \"8s/q\"},\n",
    "        \"qwen3-32b\": {\"name\": \"Qwen 3 32B\", \"vram\": 64, \"speed\": \"15s/q\"},\n",
    "        \"deepseek-r1\": {\"name\": \"DeepSeek-R1 67B\", \"vram\": 180, \"speed\": \"45s/q\", \"reasoning\": True},\n",
    "        \"mistral-large-3\": {\"name\": \"Mistral Large 3 123B\", \"vram\": 280, \"speed\": \"50s/q\"},\n",
    "    }\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt2\", lora_path=None, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        self.model_name = model_name\n",
    "        self.lora_path = lora_path\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Load model checking local paths first for offline Kaggle use\"\"\"\n",
    "        print(f\"Loading {self.SUPPORTED_MODELS.get(self.model_name, {}).get('name', self.model_name)}...\")\n",
    "        \n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        from peft import PeftModel\n",
    "        \n",
    "        # Check for local model paths (Kaggle input datasets)\n",
    "        # Priority: 1. Exact path, 2. /kaggle/input/{name}, 3. Recursive search\n",
    "        \n",
    "        def find_model_path_recursive(base_path=\"/kaggle/input\", model_identifier=\"gpt2\"):\n",
    "            \"\"\"Recursively search for model directory containing specific identifier\"\"\"\n",
    "            try:\n",
    "                # First check common paths specific to this competition/user\n",
    "                sys_paths = [\n",
    "                    f\"/kaggle/input/{model_identifier}\",\n",
    "                    f\"/kaggle/input/model-{model_identifier}\",\n",
    "                    f\"/kaggle/input/datasets/muzansano/model-{model_identifier}\",\n",
    "                    f\"/kaggle/input/datasets/muzansano/{model_identifier}\",\n",
    "                ]\n",
    "                for p in sys_paths:\n",
    "                    if os.path.exists(p):\n",
    "                        return p\n",
    "\n",
    "                # Fallback: Recursive search\n",
    "                if os.path.exists(base_path):\n",
    "                    for root, dirs, files in os.walk(base_path):\n",
    "                        if model_identifier in root.split(os.sep):\n",
    "                            return root\n",
    "                        for d in dirs:\n",
    "                            if model_identifier in d or f\"model-{model_identifier}\" in d:\n",
    "                                return os.path.join(root, d)\n",
    "            except:\n",
    "                pass\n",
    "            return None\n",
    "\n",
    "        # Try to find model path dynamically\n",
    "        found_path = find_model_path_recursive(model_identifier=self.model_name.split('/')[-1])\n",
    "        if found_path:\n",
    "             print(f\"\u2705 Dynamically found model path: {found_path}\")\n",
    "        \n",
    "        potential_paths = [\n",
    "            self.model_name,\n",
    "            found_path if found_path else f\"/kaggle/input/{self.model_name.split('/')[-1]}\",\n",
    "            f\"/kaggle/input/{self.model_name.split('/')[-1]}\",\n",
    "            f\"/kaggle/input/model-{self.model_name.split('/')[-1]}\",\n",
    "            f\"/kaggle/input/{self.model_name.replace('/', '-')}\"\n",
    "        ]\n",
    "        \n",
    "        model_path = self.model_name\n",
    "        local_files_only = False\n",
    "        \n",
    "        for path in potential_paths:\n",
    "            if os.path.exists(path):\n",
    "                print(f\"\u2705 Found local model at: {path}\")\n",
    "                model_path = path\n",
    "                local_files_only = True\n",
    "                break\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=local_files_only)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16 if self.device == \"cuda\" else torch.float32,\n",
    "                device_map=\"auto\" if self.device == \"cuda\" else None,\n",
    "                local_files_only=local_files_only\n",
    "            )\n",
    "            \n",
    "            # Load LoRA weights if available\n",
    "            if self.lora_path and os.path.exists(self.lora_path):\n",
    "                print(f\"Loading LoRA from {self.lora_path}...\")\n",
    "                self.model = PeftModel.from_pretrained(self.model, self.lora_path)\n",
    "            \n",
    "            if self.device == \"cpu\":\n",
    "                self.model = self.model.to(self.device)\n",
    "            \n",
    "            self.model.eval()\n",
    "            if self.tokenizer.pad_token is None:\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            \n",
    "            print(\"\u2705 Model loaded!\")\n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Failed to load model: {e}\")\n",
    "            raise e\n",
    "    \n",
    "    def create_prompt(self, problem, include_cot=True):\n",
    "        \"\"\"Create chain-of-thought prompt\"\"\"\n",
    "        if include_cot:\n",
    "            return f\"\"\"Solve this mathematical problem step by step.\n",
    "\n",
    "Problem: {problem}\n",
    "\n",
    "Let's think through this:\n",
    "1. What is being asked?\n",
    "2. What approach should we use?\n",
    "3. Let's work through the solution\n",
    "4. Extract the numerical answer\n",
    "\n",
    "Answer:\"\"\"\n",
    "        else:\n",
    "            return f\"Problem: {problem}\\n\\nAnswer:\"\n",
    "    \n",
    "    def extract_answer(self, text):\n",
    "        \"\"\"Extract numeric answer from generated text\"\"\"\n",
    "        # Try patterns like \"Answer: 42\"\n",
    "        match = re.search(r'(?:Answer|Final|Result)[:\\s]*([-\\d.]+)', text, re.IGNORECASE)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        \n",
    "        # Extract last number\n",
    "        numbers = re.findall(r'-?\\d+(?:\\.\\d+)?', text)\n",
    "        if numbers:\n",
    "            return numbers[-1]\n",
    "        \n",
    "        return \"0\"\n",
    "    \n",
    "    def generate(self, problem, max_length=512, temperature=0.7):\n",
    "        \"\"\"Generate answer for problem\"\"\"\n",
    "        prompt = self.create_prompt(problem, include_cot=True)\n",
    "        inputs = self.tokenizer.encode(prompt, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                inputs,\n",
    "                max_length=max_length,\n",
    "                temperature=temperature,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        answer = self.extract_answer(text.split(\"Answer:\")[-1])\n",
    "        return answer\n",
    "    \n",
    "    @staticmethod\n",
    "    def list_available_models():\n",
    "        \"\"\"List all available models\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"AVAILABLE MODELS\")\n",
    "        print(\"=\"*80)\n",
    "        for key, info in AIOMInference.SUPPORTED_MODELS.items():\n",
    "            reasoning = \" [Reasoning]\" if info.get(\"reasoning\") else \"\"\n",
    "            print(f\"{key:<20} | {info['name']:<40} | {info['vram']:<6}GB | {info['speed']}{reasoning}\")\n",
    "        print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"\u2705 Inference class updated for Offline/Kaggle use\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c51a90",
   "metadata": {},
   "source": [
    "## 4. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d869bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Loading weights: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 148/148 [00:00<00:00, 435.28it/s, Materializing param=transformer.wte.weight]             \n",
      "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Show available models\n",
    "AIOMInference.list_available_models()\n",
    "\n",
    "# Select model based on available VRAM\n",
    "selected_model = \"gpt2\"  # Change to other models based on your GPU\n",
    "# Options: \"gemma3-4b\", \"gemma3-12b\", \"llama4-scout\", \"qwen3-32b\", etc.\n",
    "\n",
    "print(f\"Selected model: {selected_model}\")\n",
    "\n",
    "# Initialize inference engine\n",
    "inferencer = AIOMInference(\n",
    "    model_name=selected_model,\n",
    "    lora_path=None,  # Set to fine-tuned model path after training\n",
    "    device=DEVICE\n",
    ")\n",
    "inferencer.load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f699fd",
   "metadata": {},
   "source": [
    "## 5. Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5365a938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from: ../datasets/aimo3_test.csv\n",
      "\u2705 Loaded 3 test problems\n",
      "       id                 problem\n",
      "0  000aaa          What is $1-1$?\n",
      "1  111bbb    What is $0\\times10$?\n",
      "2  222ccc  Solve $4+x=4$ for $x$.\n"
     ]
    }
   ],
   "source": [
    "# 5. Check environment and initialize data loader\n",
    "try:\n",
    "    import aimo\n",
    "    env = aimo.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    KAGGLE_MODE = True\n",
    "    print(\"\u2705 Kaggle AIMO environment initialized\")\n",
    "except ImportError:\n",
    "    KAGGLE_MODE = False\n",
    "    print(\"\u26a0\ufe0f AIMO API not found - running in LOCAL/DEBUG mode\")\n",
    "    \n",
    "    # Check available test data sources for local debugging\n",
    "    test_paths = [\n",
    "        \"datasets/aimo3_test.csv\",  # Local path (absolute)\n",
    "        \"../datasets/aimo3_test.csv\",  # Local path (relative)\n",
    "        \"/kaggle/input/ai-mathematical-olympiad-progress-prize-3/test.csv\"  # Kaggle public data\n",
    "    ]\n",
    "    \n",
    "    test_path = None\n",
    "    for path in test_paths:\n",
    "        if os.path.exists(path):\n",
    "            test_path = path\n",
    "            break\n",
    "            \n",
    "    if test_path:\n",
    "        print(f\"Loading local test data from: {test_path}\")\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        print(f\"\u2705 Loaded {len(test_df)} test problems\")\n",
    "    else:\n",
    "        # Create dummy data if no file found\n",
    "        print(\"\u26a0\ufe0f No test data found. Creating dummy data.\")\n",
    "        test_df = pd.DataFrame([\n",
    "            {\"id\": \"000aaa\", \"problem\": \"What is $1+1$?\"},\n",
    "            {\"id\": \"111bbb\", \"problem\": \"Solve $x^2=4$ for positive $x$.\"},\n",
    "            {\"id\": \"222ccc\", \"problem\": \"Find the sum of angles in a triangle.\"}\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cddfae",
   "metadata": {},
   "source": [
    "## 6. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa4489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for 3 problems...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [01:14<00:00, 24.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2705 Generated 3 predictions\n",
      "       id answer\n",
      "0  000aaa     48\n",
      "1  111bbb      2\n",
      "2  222ccc      5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Core Classes & Prediction Loop\n",
    "import time\n",
    "import re\n",
    "import sympy as sp\n",
    "from typing import Any, Optional, Dict, Tuple, Union\n",
    "\n",
    "# --- Core Classes (Injected for Kaggle Standalone) ---\n",
    "\n",
    "class SymbolicCompute:\n",
    "    \"\"\"SymPy-based symbolic computation and verification.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_expression(expr_str: str) -> Optional[Union[int, float]]:\n",
    "        \"\"\"Evaluate a mathematical expression string using SymPy.\"\"\"\n",
    "        try:\n",
    "            expr = sp.sympify(expr_str)\n",
    "            result = expr.evalf()\n",
    "            return int(result) if result == int(result) else float(result)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_llm_output_for_expressions(llm_text: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse LLM output to extract mathematical expressions.\"\"\"\n",
    "        result = {\n",
    "            \"expressions\": [],\n",
    "            \"values\": [],\n",
    "            \"final_value\": None\n",
    "        }\n",
    "        try:\n",
    "            # Extract expressions\n",
    "            expr_pattern = r\"(?:=|equals|is)\\s*(\\d+(?:\\.\\d+)?|[\\w\\s\\+\\-\\*\\/\\(\\)\\.]+)\"\n",
    "            expr_matches = re.findall(expr_pattern, llm_text, re.IGNORECASE)\n",
    "            \n",
    "            # Extract numeric values\n",
    "            num_pattern = r\"\\b(\\d+(?:\\.\\d+)?)\\b\"\n",
    "            num_matches = re.findall(num_pattern, llm_text)\n",
    "            \n",
    "            result[\"expressions\"] = expr_matches[:5]\n",
    "            result[\"values\"] = [float(n) if '.' in n else int(n) for n in num_matches[:10]]\n",
    "            \n",
    "            if num_matches:\n",
    "                result[\"final_value\"] = float(num_matches[-1]) if '.' in num_matches[-1] else int(num_matches[-1])\n",
    "        except:\n",
    "            pass\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def verify_symbolic_result(\n",
    "        llm_answer: int,\n",
    "        llm_output: str,\n",
    "        tolerance: float = 0.01\n",
    "    ) -> Tuple[bool, float]:\n",
    "        \"\"\"Verify LLM answer by symbolic computation. Returns (is_valid, confidence).\"\"\"\n",
    "        try:\n",
    "            parsed = SymbolicCompute.parse_llm_output_for_expressions(llm_output)\n",
    "            \n",
    "            # Try to evaluate extracted expressions\n",
    "            if parsed[\"expressions\"]:\n",
    "                for expr_str in parsed[\"expressions\"]:\n",
    "                    try:\n",
    "                        result = SymbolicCompute.evaluate_expression(expr_str)\n",
    "                        if result is not None:\n",
    "                            if isinstance(result, float):\n",
    "                                diff_percent = abs(result - llm_answer) / max(abs(llm_answer), 1)\n",
    "                                if diff_percent <= tolerance:\n",
    "                                    return True, 1.0 - diff_percent\n",
    "                            else:\n",
    "                                if int(result) == llm_answer:\n",
    "                                    return True, 1.0\n",
    "                    except:\n",
    "                        continue\n",
    "            \n",
    "            # Check final value\n",
    "            if parsed[\"final_value\"] is not None and parsed[\"final_value\"] == llm_answer:\n",
    "                return True, 0.8\n",
    "                    \n",
    "        except:\n",
    "            pass\n",
    "        return False, 0.5\n",
    "\n",
    "class AnswerValidator:\n",
    "    \"\"\"Validates and enforces answer format constraints.\"\"\"\n",
    "    \n",
    "    AIMO_MIN = 0\n",
    "    AIMO_MAX = 99999\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_integer(value: Any) -> Optional[int]:\n",
    "        \"\"\"Validate and convert value to valid AIMO integer.\"\"\"\n",
    "        try:\n",
    "            int_value = int(float(str(value).strip()))\n",
    "            if int_value < AnswerValidator.AIMO_MIN: return AnswerValidator.AIMO_MIN\n",
    "            if int_value > AnswerValidator.AIMO_MAX: return AnswerValidator.AIMO_MAX\n",
    "            return int_value\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def validate_with_fallback_strategies(\n",
    "        llm_answer: Optional[int],\n",
    "        llm_text: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Validate answer with multiple fallback strategies.\"\"\"\n",
    "        result = {\n",
    "            \"final_answer\": 0,\n",
    "            \"confidence\": 0.0,\n",
    "            \"strategy_used\": \"default_fallback\",\n",
    "            \"fallback_applied\": False\n",
    "        }\n",
    "        try:\n",
    "            # Strategy 1: Use primary answer if valid\n",
    "            if llm_answer is not None:\n",
    "                validated = AnswerValidator.validate_integer(llm_answer)\n",
    "                if validated is not None:\n",
    "                    result[\"final_answer\"] = validated\n",
    "                    result[\"confidence\"] = 0.9\n",
    "                    result[\"strategy_used\"] = \"primary_llm_answer\"\n",
    "                    return result\n",
    "            \n",
    "            # Strategy 2: Try symbolic verification\n",
    "            is_valid, confidence = SymbolicCompute.verify_symbolic_result(\n",
    "                llm_answer if llm_answer is not None else 0,\n",
    "                llm_text\n",
    "            )\n",
    "            if is_valid and llm_answer is not None:\n",
    "                validated = AnswerValidator.validate_integer(llm_answer)\n",
    "                if validated is not None:\n",
    "                    result[\"final_answer\"] = validated\n",
    "                    result[\"confidence\"] = confidence\n",
    "                    result[\"strategy_used\"] = \"symbolic_verification\"\n",
    "                    result[\"fallback_applied\"] = True\n",
    "                    return result\n",
    "            \n",
    "            result[\"fallback_applied\"] = True\n",
    "        except:\n",
    "            pass\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def handle_edge_cases(answer: int) -> Tuple[int, str]:\n",
    "        \"\"\"Handle edge cases in answer validation.\"\"\"\n",
    "        try:\n",
    "            if answer < 0: return 0, \"Negative answer converted to 0\"\n",
    "            if answer > AnswerValidator.AIMO_MAX * 10: \n",
    "                return AnswerValidator.AIMO_MAX, \"Very large answer capped\"\n",
    "            validated = AnswerValidator.validate_integer(answer)\n",
    "            return validated if validated is not None else 0, \"\"\n",
    "        except:\n",
    "            return 0, \"Edge case error\"\n",
    "\n",
    "class ExecutionMetrics:\n",
    "    \"\"\"Track execution metrics for the pipeline.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            \"total\": 0, \"successful\": 0, \"failed\": 0, \n",
    "            \"fallback\": 0, \"verified\": 0\n",
    "        }\n",
    "    \n",
    "    def record_result(self, success: bool, fallback_used: bool = False, verified: bool = False):\n",
    "        self.metrics[\"total\"] += 1\n",
    "        if success: self.metrics[\"successful\"] += 1\n",
    "        else: self.metrics[\"failed\"] += 1\n",
    "        if fallback_used: self.metrics[\"fallback\"] += 1\n",
    "        if verified: self.metrics[\"verified\"] += 1\n",
    "\n",
    "# --- Global Instances ---\n",
    "symbolic_compute = SymbolicCompute()\n",
    "answer_validator = AnswerValidator()\n",
    "execution_metrics = ExecutionMetrics()\n",
    "inference_engine = AIOMInference(model_name=\"gpt2\") # Initialize inference\n",
    "\n",
    "try:\n",
    "    inference_engine.load_model()\n",
    "except Exception as e:\n",
    "    print(f\"\u26a0\ufe0f Inference engine load failed (expected in CI/CD without full weights): {e}\")\n",
    "\n",
    "# --- Prediction Logic ---\n",
    "\n",
    "predictions = []\n",
    "\n",
    "def solve_one_problem(problem_id, problem_text):\n",
    "    \"\"\"Solve a single problem and return the answer with verification.\"\"\"\n",
    "    try:\n",
    "        # 1. Generate initial answer\n",
    "        answer_str = inference_engine.generate(problem_text)\n",
    "        \n",
    "        # 2. Extract numeric candidate\n",
    "        initial_answer = 0\n",
    "        try:\n",
    "            initial_answer = int(float(answer_str))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # 3. Apply Verification & Validation\n",
    "        # Symbolic verification\n",
    "        is_valid, confidence = symbolic_compute.verify_symbolic_result(initial_answer, str(problem_text))\n",
    "        \n",
    "        # Fallback validation if needed\n",
    "        if not is_valid:\n",
    "            validation_result = answer_validator.validate_with_fallback_strategies(initial_answer, str(problem_text))\n",
    "            final_answer = validation_result.get('final_answer', initial_answer)\n",
    "            fallback_used = True\n",
    "        else:\n",
    "            final_answer = initial_answer\n",
    "            fallback_used = False\n",
    "        \n",
    "        # 4. Final edge case check\n",
    "        final_answer, note = answer_validator.handle_edge_cases(final_answer)\n",
    "        \n",
    "        # 5. Record metrics\n",
    "        execution_metrics.record_result(success=True, verified=is_valid, fallback_used=fallback_used)\n",
    "        \n",
    "        return final_answer\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error on {problem_id}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# Main Execution Loop\n",
    "if KAGGLE_MODE:\n",
    "    print(\"\ud83d\ude80 Running in KAGGLE SUBMISSION MODE\")\n",
    "    for (test, sample_prediction) in iter_test:\n",
    "        test_row = test.iloc[0]\n",
    "        problem_id = str(test_row['id'])\n",
    "        problem_text = str(test_row['problem'])\n",
    "        \n",
    "        pred = solve_one_problem(problem_id, problem_text)\n",
    "        sample_prediction['answer'] = pred\n",
    "        env.predict(sample_prediction)\n",
    "        \n",
    "    print(\"\u2705 Submission loop complete\")\n",
    "    \n",
    "else:\n",
    "    print(\"\ud83d\udd27 Running in LOCAL DEBUG MODE\")\n",
    "    local_preds = []\n",
    "    for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "        problem_id = str(row['id'])\n",
    "        problem_text = str(row['problem'])\n",
    "        \n",
    "        pred = solve_one_problem(problem_id, problem_text)\n",
    "        local_preds.append({'id': problem_id, 'answer': pred})\n",
    "        \n",
    "    submission_df = pd.DataFrame(local_preds)\n",
    "    print(f\"\u2705 Generated {len(submission_df)} local predictions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32460901",
   "metadata": {},
   "source": [
    "## 7. Save Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c03805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save Submission (Only needed for local mode)\n",
    "if not KAGGLE_MODE:\n",
    "    submission_path = \"submission.csv\"\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    \n",
    "    print(f\"\u2705 Submission saved to {submission_path}\")\n",
    "    print(f\"\\nFile size: {os.path.getsize(submission_path)} bytes\")\n",
    "    print(f\"Rows: {len(submission_df)}\")\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(submission_df.head())\n",
    "else:\n",
    "    print(\"\u2705 Submission file generated via AIMO API\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804e2bcd",
   "metadata": {},
   "source": [
    "## 8. Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f2928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 4: Export verification metrics and analysis\n",
    "if PHASE4_AVAILABLE:\n",
    "    print(\"\ud83d\udcca Phase 4 Metrics Export\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get execution metrics\n",
    "    metrics_summary = execution_metrics.get_summary()\n",
    "    print(f\"\\nExecution Metrics:\")\n",
    "    for key, value in metrics_summary.items():\n",
    "        if isinstance(value, float):\n",
    "            if 'rate' in key:\n",
    "                print(f\"  {key}: {value:.1%}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Get verification statistics\n",
    "    verification_stats = verification_tracker.get_summary_statistics()\n",
    "    print(f\"\\nVerification Statistics:\")\n",
    "    for key, value in verification_stats.items():\n",
    "        if isinstance(value, float):\n",
    "            if 'rate' in key:\n",
    "                print(f\"  {key}: {value:.1%}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value:.2f}\")\n",
    "        else:\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    metrics_output = {\n",
    "        'execution_metrics': metrics_summary,\n",
    "        'verification_stats': verification_stats\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    metrics_file = 'phase4_metrics.json'\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        json.dump(metrics_output, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\n\u2705 Metrics saved to {metrics_file}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f Phase 4 metrics not available (verification disabled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2359bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify submission format\n",
    "print(\"\u2705 SUBMISSION VERIFICATION:\")\n",
    "print(f\"- Columns: {list(submission_df.columns)} (expected: ['id', 'answer'])\")\n",
    "print(f\"- Row count: {len(submission_df)}\")\n",
    "print(f\"- No missing values: {not submission_df.isnull().any().any()}\")\n",
    "print(f\"- Answer examples: {submission_df['answer'].head().tolist()}\")\n",
    "print(f\"\\n\u2705 Submission ready for upload!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# COMPATIBILITY FIX: Generate submission.parquet if required\n",
    "try:\n",
    "    if os.path.exists('submission.csv'):\n",
    "        # Convert API output to parquet\n",
    "        sub_df = pd.read_csv('submission.csv')\n",
    "        sub_df.to_parquet('submission.parquet')\n",
    "        print(\"\u2705 Converted submission.csv to submission.parquet\")\n",
    "    elif 'submission_df' in locals():\n",
    "        # Save local df\n",
    "        submission_df.to_parquet('submission.parquet')\n",
    "        print(\"\u2705 Saved local submission_df to submission.parquet\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f Could not generate submission.parquet: Source data missing\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error generating submission.parquet: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# COMPATIBILITY FIX: Generate submission.parquet if required\n",
    "try:\n",
    "    # Check for parquet dependencies\n",
    "    try:\n",
    "        import pyarrow\n",
    "    except ImportError:\n",
    "        try:\n",
    "            import fastparquet\n",
    "        except ImportError:\n",
    "            print(\"\u26a0\ufe0f No parquet engine found (pyarrow/fastparquet). Skipping parquet generation.\")\n",
    "            sub_df = None\n",
    "    \n",
    "    if 'sub_df' in locals() or os.path.exists('submission.csv') or 'submission_df' in locals():\n",
    "        df_to_save = None\n",
    "        if 'submission_df' in locals():\n",
    "            df_to_save = submission_df\n",
    "        elif os.path.exists('submission.csv'):\n",
    "            df_to_save = pd.read_csv('submission.csv')\n",
    "        \n",
    "        if df_to_save is not None and not df_to_save.empty:\n",
    "            try:\n",
    "                df_to_save.to_parquet('submission.parquet')\n",
    "                print(\"\u2705 Generated submission.parquet\")\n",
    "            except Exception as pe:\n",
    "                print(f\"\u26a0\ufe0f Parquet export failed (likely missing engine): {pe}\")\n",
    "        else:\n",
    "            print(\"\u26a0\ufe0f No data to save to parquet.\")\n",
    "except Exception as e:\n",
    "    print(f\"\u274c Error in parquet generation block: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}